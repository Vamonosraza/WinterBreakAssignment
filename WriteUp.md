## Project Overview

For this project I used Scrapy to help me scrape for data from the recommended sites. Scrapy is a powerful scraping framework for Python. I had never used it so it was fun learning a software while finishing this assignment. My goal was to gather information from various Amazon and Best Buy products, clean the data, calculate savings, and visualize the results using pandas and seaborn. Scrapy allowed me to define 'spiders', which are self-contained crawlers that follow links and extract data according to the rules I specify. In this project I created two spiders: one for Amazon and one for best Buy. The amazon spider (`amazon_spider.py`) navigates to Amazon and search results for laptops, extracts the product name, price, true price, and URL for each product, then saves the data in a JSON file (`amazon_products.json`). I do the same with the Best Buy spider (`bestbuy_spider.py`) to get similar information for their headphone products (`bestbuy_headphones.json`). Using this data stored in the JSOn files, I use pandas to load and clean the data by removing rows with null values. The `pandas_scrap.py` script reads the JSOn files into padas DataFrames, removes the null rows, and converts the price columns from text to numeric types. This allows for consistent format for later analysis. To calculate the average savings for each site I subtract the current price from the true price and add the results to a new column called 'Savings' to each DataFrame and get the mean of the column. I also decided to find the products with the biggest savings. I was able to do this using the `idmax()` function to get the index of the maximum savings and retrieve the corresponding row. To plot the results I used seaborn and matplot. I visualized it with a bar plot to compare the average savings on each site. The plot provides a clear visual representation of the savings, making it easy to compare the two sites. This was a short assignment that introduce new frameworks for me but i learn how powerful Scrapy is for web scraping, pandas for data cleaning and analysis, and seaborn for data visualiztion. Together these tools can extract valuable insights from web data and present them in an informative and visually appealing manner.